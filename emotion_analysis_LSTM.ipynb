{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOw37i17abGabIGbY5KPoF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymuto0302/RW2025/blob/main/emotion_analysis_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ImDB をターゲットとした感情分析"
      ],
      "metadata": {
        "id": "FMhds8YDLETv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from collections import Counter\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIs8jPxvJ5aU",
        "outputId": "a0dfe56c-a71d-4b65-b841-b08385b34293"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDbDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab_to_idx, max_len=500):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # テキストをインデックスに変換\n",
        "        tokens = self.preprocess_text(text)\n",
        "        indices = [self.vocab_to_idx.get(token, self.vocab_to_idx['<UNK>']) for token in tokens]\n",
        "\n",
        "        # 長さ制限\n",
        "        if len(indices) > self.max_len:\n",
        "            indices = indices[:self.max_len]\n",
        "\n",
        "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # テキストの前処理\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        tokens = text.split()\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "M68jmbQ6J8O_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMSentimentAnalyzer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
        "        super(LSTMSentimentAnalyzer, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
        "                           batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # bidirectional\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        # x: (batch_size, seq_len)\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # 最後の隠れ状態を使用（双方向なので結合）\n",
        "        # hidden: (num_layers * 2, batch_size, hidden_dim)\n",
        "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)  # (batch_size, hidden_dim * 2)\n",
        "\n",
        "        # ドロップアウト\n",
        "        hidden = self.dropout(hidden)\n",
        "\n",
        "        # 分類層\n",
        "        output = self.fc(hidden)  # (batch_size, num_classes)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "gRzyxBe4KA5n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 諸々のヘルパー関数\n",
        "\n",
        "def build_vocab(texts, min_freq=2, max_vocab_size=10000):\n",
        "    \"\"\"語彙を構築\"\"\"\n",
        "    word_counts = Counter()\n",
        "\n",
        "    for text in texts:\n",
        "        tokens = preprocess_text(text)\n",
        "        word_counts.update(tokens)\n",
        "\n",
        "    # 頻度でソート\n",
        "    vocab_items = word_counts.most_common(max_vocab_size - 4)  # 特殊トークン分を除く\n",
        "\n",
        "    # 語彙辞書を作成\n",
        "    vocab_to_idx = {\n",
        "        '<PAD>': 0,\n",
        "        '<UNK>': 1,\n",
        "        '<SOS>': 2,\n",
        "        '<EOS>': 3\n",
        "    }\n",
        "\n",
        "    for word, freq in vocab_items:\n",
        "        if freq >= min_freq:\n",
        "            vocab_to_idx[word] = len(vocab_to_idx)\n",
        "\n",
        "    return vocab_to_idx\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"テキストの前処理\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"バッチ処理用の関数\"\"\"\n",
        "    texts, labels = zip(*batch)\n",
        "\n",
        "    # パディング\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "def load_imdb_data():\n",
        "    \"\"\"IMDbデータセットを読み込み（実際の実装では適切なデータ読み込みを行う）\"\"\"\n",
        "    # この部分は実際のデータセットに応じて実装\n",
        "    # 例: torchtext.datasets.IMDB() を使用するか、\n",
        "    # 手動でファイルから読み込む\n",
        "\n",
        "    # サンプルデータ（実際の実装では置き換える）\n",
        "    sample_texts = [\n",
        "        \"This movie was absolutely wonderful! Great acting and amazing plot.\",\n",
        "        \"Terrible film. Waste of time and money. Very disappointed.\",\n",
        "        \"Good movie with excellent cinematography and strong performances.\",\n",
        "        \"Boring and predictable. Not worth watching.\",\n",
        "        \"Outstanding film! Highly recommend to everyone.\",\n",
        "        \"Poor acting and weak storyline. Not impressed.\"\n",
        "    ]\n",
        "\n",
        "    sample_labels = [1, 0, 1, 0, 1, 0]  # 1: positive, 0: negative\n",
        "\n",
        "    # 実際の実装では以下のようにデータを読み込む\n",
        "    # from torchtext.datasets import IMDB\n",
        "    # train_iter, test_iter = IMDB(split=('train', 'test'))\n",
        "\n",
        "    return sample_texts, sample_labels\n"
      ],
      "metadata": {
        "id": "btM3cbZRKM57"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"1エポックの訓練\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for batch_idx, (texts, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 予測値を取得\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, f1\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    \"\"\"モデルの評価\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, f1, predictions, true_labels"
      ],
      "metadata": {
        "id": "4Et8o6AOKXmJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## メイン実行部分"
      ],
      "metadata": {
        "id": "KRZyQBg0Kdm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def main():\n",
        "    # ハイパーパラメータ\n",
        "    EMBEDDING_DIM = 128\n",
        "    HIDDEN_DIM = 128\n",
        "    NUM_LAYERS = 2\n",
        "    NUM_CLASSES = 2\n",
        "    DROPOUT = 0.5\n",
        "    BATCH_SIZE = 32\n",
        "    LEARNING_RATE = 0.001\n",
        "    NUM_EPOCHS = 10\n",
        "    MAX_LEN = 500\n",
        "\n",
        "    # データ読み込み\n",
        "    print(\"データを読み込み中...\")\n",
        "    texts, labels = load_imdb_data()\n",
        "\n",
        "    # 実際の実装では train/test 分割を行う\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "\n",
        "    # 動作チェック用\n",
        "    #train_texts, train_labels = texts[:4], labels[:4]\n",
        "    #test_texts, test_labels = texts[4:], labels[4:]\n",
        "\n",
        "    # 語彙構築\n",
        "    vocab_to_idx = build_vocab(train_texts)\n",
        "    vocab_size = len(vocab_to_idx)\n",
        "    print(f\"語彙サイズ: {vocab_size}\")\n",
        "\n",
        "    # データセット作成\n",
        "    train_dataset = IMDbDataset(train_texts, train_labels, vocab_to_idx, MAX_LEN)\n",
        "    test_dataset = IMDbDataset(test_texts, test_labels, vocab_to_idx, MAX_LEN)\n",
        "\n",
        "    # データローダー作成\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # モデル作成\n",
        "    model = LSTMSentimentAnalyzer(vocab_size, EMBEDDING_DIM, HIDDEN_DIM,\n",
        "                                 NUM_LAYERS, NUM_CLASSES, DROPOUT)\n",
        "    model.to(device)\n",
        "\n",
        "    # 損失関数とオプティマイザ\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # 訓練\n",
        "    print(\"訓練開始...\")\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # 訓練\n",
        "        train_loss, train_acc, train_f1 = train_model(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # 評価\n",
        "        test_loss, test_acc, test_f1, predictions, true_labels = evaluate_model(model, test_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
        "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # ベストモデルの保存\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
        "            print(f\"ベストモデルを保存しました (F1: {best_f1:.4f})\")\n",
        "\n",
        "    # 最終評価\n",
        "    print(\"\\n最終評価:\")\n",
        "    print(classification_report(true_labels, predictions, target_names=['Negative', 'Positive']))\n",
        "\n",
        "    # 語彙の保存\n",
        "    with open('vocab.pkl', 'wb') as f:\n",
        "        pickle.dump(vocab_to_idx, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVWToz84KmYW",
        "outputId": "813002a3-db31-495b-9b49-6d8577057e91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データを読み込み中...\n",
            "語彙サイズ: 7\n",
            "訓練開始...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 34.15it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 115.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.6723, Train Acc: 0.7500, Train F1: 0.7333\n",
            "Test Loss: 0.6917, Test Acc: 0.5000, Test F1: 0.3333\n",
            "------------------------------------------------------------\n",
            "ベストモデルを保存しました (F1: 0.3333)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 39.55it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 221.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "Train Loss: 0.6522, Train Acc: 1.0000, Train F1: 1.0000\n",
            "Test Loss: 0.6815, Test Acc: 1.0000, Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "ベストモデルを保存しました (F1: 1.0000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 34.81it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 226.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10\n",
            "Train Loss: 0.7007, Train Acc: 0.5000, Train F1: 0.3333\n",
            "Test Loss: 0.6710, Test Acc: 0.5000, Test F1: 0.3333\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 37.44it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 81.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10\n",
            "Train Loss: 0.6576, Train Acc: 0.7500, Train F1: 0.7333\n",
            "Test Loss: 0.6604, Test Acc: 0.5000, Test F1: 0.3333\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 54.12it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 236.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10\n",
            "Train Loss: 0.6718, Train Acc: 0.5000, Train F1: 0.3333\n",
            "Test Loss: 0.6516, Test Acc: 0.5000, Test F1: 0.3333\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 45.63it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 42.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10\n",
            "Train Loss: 0.6479, Train Acc: 0.7500, Train F1: 0.7333\n",
            "Test Loss: 0.6432, Test Acc: 0.5000, Test F1: 0.3333\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 34.73it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 214.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10\n",
            "Train Loss: 0.6515, Train Acc: 0.7500, Train F1: 0.7333\n",
            "Test Loss: 0.6341, Test Acc: 1.0000, Test F1: 1.0000\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 25.36it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 220.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10\n",
            "Train Loss: 0.5942, Train Acc: 1.0000, Train F1: 1.0000\n",
            "Test Loss: 0.6234, Test Acc: 1.0000, Test F1: 1.0000\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 54.64it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 219.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10\n",
            "Train Loss: 0.5554, Train Acc: 1.0000, Train F1: 1.0000\n",
            "Test Loss: 0.6128, Test Acc: 1.0000, Test F1: 1.0000\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1/1 [00:00<00:00, 20.08it/s]\n",
            "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 150.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10\n",
            "Train Loss: 0.5830, Train Acc: 0.7500, Train F1: 0.7333\n",
            "Test Loss: 0.6052, Test Acc: 1.0000, Test F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "最終評価:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      1.00      1.00         1\n",
            "    Positive       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJlBvTmzXbDZ",
        "outputId": "55a23aa9-7be7-4cea-de40-98dfc1f4d031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: This movie was absolutely amazing! I loved every minute of it.\n",
            "Sentiment: Positive (Confidence: 0.5039)\n"
          ]
        }
      ],
      "source": [
        "def predict_sentiment(model, text, vocab_to_idx, device, max_len=500):\n",
        "    \"\"\"単一テキストの感情予測\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # テキストの前処理\n",
        "    tokens = preprocess_text(text)\n",
        "    indices = [vocab_to_idx.get(token, vocab_to_idx['<UNK>']) for token in tokens]\n",
        "\n",
        "    if len(indices) > max_len:\n",
        "        indices = indices[:max_len]\n",
        "\n",
        "    # テンソルに変換\n",
        "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    sentiment = \"Positive\" if predicted.item() == 1 else \"Negative\"\n",
        "    confidence = probabilities[0][predicted.item()].item()\n",
        "\n",
        "    return sentiment, confidence\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 保存されたモデルと語彙を読み込み\n",
        "    with open('vocab.pkl', 'rb') as f:\n",
        "        vocab_to_idx = pickle.load(f)\n",
        "\n",
        "    model = LSTMSentimentAnalyzer(len(vocab_to_idx), 128, 128, 2, 2, 0.5)\n",
        "    model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
        "    model.to(device)\n",
        "\n",
        "    # テストテキスト\n",
        "    test_text = \"This movie was absolutely amazing! I loved every minute of it.\"\n",
        "    sentiment, confidence = predict_sentiment(model, test_text, vocab_to_idx, device)\n",
        "    print(f\"Text: {test_text}\")\n",
        "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vP5hKOlNLofj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}